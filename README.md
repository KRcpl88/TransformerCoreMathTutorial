# TransformerCoreMathTutorial
A tutorial on the core math behind the multi headed attention tranformer block 
