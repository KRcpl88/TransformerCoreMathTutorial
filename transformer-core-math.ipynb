{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### https://github.com/KRcpl88/GptTransformerTutorial"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Transformer Tutorial Generated by GPT 4\n",
        "\n",
        "![Transformer](Optimusprime-originaltoy.jpg)\n",
        "\n",
        "Both the descriptive explanations and the code samples for this tutorial were generated entirely with chatGPT using the GPT 4 model. In some cases the initial code had minor errors, these errors were also fixed by GPT 4 by feeding the errors back into GPT 4 and GPT 4 would generate new code.\n",
        "\n",
        "This is an advanced tutorial which builds the main components of the Transformer model, the multi headed attention mechanism and the position and token embedding, from scratch in PyTorch.\n",
        "\n",
        "#### Prompt: \n",
        "```\n",
        "How can I build a transformer model from scratch using IMDB and pytorch\n",
        "```\n",
        "\n",
        "\n",
        "## IMDB Sentiment Analysis\n",
        "\n",
        "The Keras IMDB dataset is a popular dataset for sentiment analysis tasks in natural language processing (NLP). It contains 50,000 movie reviews from the Internet Movie Database (IMDB) labeled as either positive (1) or negative (0) based on the sentiment expressed in the review. The dataset is divided into 25,000 reviews for training and 25,000 reviews for testing.\n",
        "\n",
        "The reviews in the dataset have been preprocessed, and each review is encoded as a sequence of word indices (integers). The indices represent the overall frequency rank of the words in the entire dataset. For instance, the integer \"3\" encodes the 3rd most frequent word in the data. This encoding allows for faster processing and less memory usage compared to working with raw text data.\n",
        "\n",
        "The Keras IMDB dataset is typically used for binary classification tasks, where the goal is to build a machine learning model that can predict whether a given movie review is positive or negative based on the text content. The dataset is accessible through the tensorflow.keras.datasets module in the TensorFlow library.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706552974849
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Headed attention\n",
        "\n",
        "This class takes as input the model dimension d_model and the number of attention heads num_heads. The forward method takes a tensor of shape (batch_size, sequence_length, d_model) and an optional mask, and it outputs the context vectors and attention weights."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.W_Queries = nn.Linear(d_model, d_model)\n",
        "        self.W_Keys = nn.Linear(d_model, d_model)\n",
        "        self.W_Values = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.FullyConnectedLayer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Queries, Keys, Values, mask=None):\n",
        "        attention_logits = torch.matmul(Queries, Keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            attention_logits = attention_logits.masked_fill(mask == 0, float('-inf'))\n",
        "        attention_weights = F.softmax(attention_logits, dim=-1)\n",
        "        return torch.matmul(attention_weights, Values), attention_weights\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        return x.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_len, _ = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        Queries = self.split_heads(self.W_Queries(x))\n",
        "        Keys = self.split_heads(self.W_Keys(x))\n",
        "        Values = self.split_heads(self.W_Values(x))\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "        context_vectors, attention_weights = self.scaled_dot_product_attention(Queries, Keys, Values, mask)\n",
        "        context_vectors = self.combine_heads(context_vectors)\n",
        "\n",
        "        return self.FullyConnectedLayer(context_vectors), attention_weights\n",
        "\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "vocab_size = 20000\n",
        "max_seq_len = 200\n",
        "\n",
        "# Example usage:\n",
        "input_tensor = torch.rand(16, 50, d_model)  # 16 is batch_size and 50 is sequence length\n",
        "\n",
        "self_attention = MultiHeadSelfAttention(d_model, num_heads)\n",
        "output, attention_weights = self_attention(input_tensor)\n",
        "\n",
        "#Enumerate the MultiHeadSelfAttention layers\n",
        "for i, layer in enumerate(self_attention.children()):\n",
        "    print(f\"Layer {i}: {layer}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706553170593
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## What is the purpose of Queries, Keys, and Values and how are they different from a simple densely connected layer?\n",
        "\n",
        "The multi-head self-attention mechanism is a crucial component, characterized by three key elements: Queries (Q), Keys (K), and Values (V). Let's explore the purpose of each and how they differ from a simple densely connected (fully connected) neural network layer.\n",
        "\n",
        "### Queries (Q), Keys (K), and Values (V)\n",
        "\n",
        "1. **Queries (Q):** \n",
        "Represent the current word (or token) for which we are trying to establish its context and relationships with other words in the input sequence.\n",
        "\n",
        "1. **Keys (K):**\n",
        "Represent all words (or tokens) in the input sequence. The model uses them to determine how much focus or 'attention' each word in the sequence should get in relation to the current query word.\n",
        "\n",
        "1. **Values (V):**\n",
        "Also represent all words in the input sequence, but they are used to construct the output of the self-attention layer. The amount of attention a word gets influences how much its corresponding value contributes to the output.\n",
        "\n",
        "#### How They Work:\n",
        "\n",
        "In the self-attention mechanism, each word in the input sequence is initially transformed into Q, K, and V vectors through distinct linear transformations (learnable weights).\n",
        "The model calculates the attention scores by performing a dot product of the Q vector with all K vectors. These scores determine how much each word in the sequence should contribute to the representation of the current word.\n",
        "The attention scores are then used to create a weighted sum of the V vectors, which forms the output of the self-attention layer for each word.\n",
        "\n",
        "### Difference from a Densely Connected Layer:\n",
        "\n",
        "A densely connected layer learns a fixed transformation of its input data, applying the same transformation to all inputs. In contrast, the self-attention mechanism dynamically calculates how much each part of the input should contribute to the output based on the input data itself.\n",
        "\n",
        "The self-attention mechanism can capture relationships and dependencies between words in a sequence, regardless of their distance from each other. A densely connected layer lacks this contextual awareness and processes each input independently.\n",
        "\n",
        "Self-attention allows the model to focus on different parts of the input sequence differently for each output element, enabling a more nuanced and context-aware processing. Densely connected layers don't offer this level of flexibility as they apply the same transformation to all inputs.\n",
        "\n",
        "### Summary\n",
        "In a multi-head self-attention function, Queries, Keys, and Values are used to dynamically compute how different parts of the input sequence should be emphasized or 'attended to' for each element in the sequence. This differs from a simple densely connected layer, which lacks the ability to capture sequential and contextual relationships within the input data. Self-attention is inherently more flexible and context-aware, making it well-suited for tasks involving sequential data, like natural language processing."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  What is the purpose of the FullyConnectedLayer layer\n",
        "\n",
        "In a Transformer's multi-head self-attention mechanism, the fourth layer, commonly referred to as the FullyConnectedLayer (fc) or sometimes as a linear layer, plays a vital role in integrating and refining the outputs from the self-attention process. Let's break down its purpose:\n",
        "\n",
        "### Purpose of the FullyConnectedLayer (fc)\n",
        "1. **Integration of Attention Heads:**\n",
        "After the self-attention mechanism processes the input through multiple heads, the results from each head need to be integrated. The FullyConnectedLayer serves to combine these diverse attention outputs into a single, unified output.\n",
        "\n",
        "1. **Transformation of Concatenated Outputs:**\n",
        "The outputs of the multiple attention heads are concatenated to form a single matrix. The FullyConnectedLayer then applies a linear transformation to this concatenated matrix. This step is crucial for mapping the combined, multi-dimensional attention information back into the original input space (or to a desired output dimensionality).\n",
        "\n",
        "1. **Maintaining Depth of Representation:**\n",
        "The FullyConnectedLayer (fc) ensures that the depth of the model's representation (i.e., the dimensionality of the feature space) is maintained or appropriately transformed. This consistency is essential for stacking multiple layers of the Transformer, allowing each layer to build upon the transformed representations of the previous layer.\n",
        "\n",
        "1. **Adding Learnable Parameters:**\n",
        "The FullyConnectedLayer (fc) introduces additional learnable parameters to the model. These parameters are optimized during training, allowing the model to better integrate and interpret the information gleaned from the multiple attention heads.\n",
        "\n",
        "1. **Enhancing Model's Capacity:** By combining and transforming the outputs of the attention heads, the FullyConnectedLayer (fc) enhances the model's capacity to capture complex patterns and relationships in the data. This step is critical for the overall performance of the Transformer in tasks like language understanding and generation.\n",
        "\n",
        "### How the FullyConnectedLayer (fc) Layer Works\n",
        "- **Linear Transformation:** The FullyConnectedLayer (fc) typically performs a linear transformation. It takes the concatenated outputs from the attention heads and multiplies them with a weight matrix (learnable parameters), often followed by adding a bias term.\n",
        "\n",
        "- **Dimensionality Management:** The FullyConnectedLayer (fc) can either preserve the dimensionality of the input or transform it to a different dimensionality, depending on the design of the Transformer model. This flexibility allows the model to be tailored to specific tasks or requirements.\n",
        "\n",
        "### Summary\n",
        "The FullyConnectedLayer (fc) in a Transformer's multi-head self-attention mechanism serves as a critical component for integrating, transforming, and refining the outputs from the attention heads. It adds depth and capacity to the model, enabling complex feature integration and aiding in the model's overall ability to process and understand sequential data effectively."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What does the split_heads function do and how does it work?\n",
        "\n",
        "The multi-head self-attention mechanism involves a function often called split_heads or a similar variant. This function is essential for enabling the \"multi-head\" aspect of the self-attention. Let's delve into what this function does and how it works:\n",
        "\n",
        "### Purpose of split_heads\n",
        "The primary purpose of `split_heads` is to enable the model to simultaneously attend to information from different representation subspaces at different positions. By splitting the attention mechanism into multiple heads, the model can capture a richer variety of features in the input data.\n",
        "\n",
        "Each head in the multi-head attention can potentially focus on different aspects of the input data, allowing for parallel and diverse feature extraction. This leads to a more comprehensive understanding of the input.\n",
        "\n",
        "### How split_heads Works\n",
        "1. **Input to the Function:**\n",
        "    - The function typically takes the matrices Queries, Keys, and Values as inputs. Each of these matrices is the result of transforming the input sequence through different linear layers specific for Queries, Keys, and Values.\n",
        "\n",
        "1. **Reshaping the Matrices:**\n",
        "    - The `split_heads` function reshapes each of Queries, Keys, and Values matrices from their original shape `[batch_size, sequence_length, feature_dimension]` to a new shape `[batch_size, num_heads, sequence_length, feature_dimension/num_heads]`.\n",
        "\n",
        "    - This reshaping effectively splits the last dimension (feature_dimension) into two dimensions: the number of heads (num_heads) and the reduced feature dimension for each head.\n",
        "\n",
        "1. **Parallel Attention Processing:**\n",
        "\n",
        "    - After splitting, each head processes a slice of the original feature dimension, allowing the model to attend to different parts of the feature space independently and in parallel.\n",
        "    - This parallel processing enables the model to capture different types of relationships in the data, such as different aspects of semantic meaning in a language model.\n",
        "\n",
        "1. **Recombination and Output:**\n",
        "    - Once each head has processed its respective slice, the outputs are typically concatenated back together and passed through another linear layer to combine the information from all heads.\n",
        "\n",
        "    - This recombination ensures that the multi-head attention captures a wide range of information from the input while still being able to integrate these diverse signals.\n",
        "\n",
        "### Summary\n",
        "The split_heads function in a Transformer's multi-head self-attention mechanism plays a crucial role in diversifying the attention process. By splitting the Queries, Keys, and Values matrices into multiple heads, the Transformer can process the input data in parallel across different feature subspaces, enhancing its ability to capture complex patterns and relationships in the data. This functionality is fundamental to the Transformer architecture's success in various tasks like language understanding, translation, and generation."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token and Position Embedding\n",
        "\n",
        "This class takes as input the vocabulary size vocab_size, the model dimension d_model, and the maximum sequence length max_seq_len. The forward method takes a tensor of shape (batch_size, sequence_length) with token ids and outputs the combined token and position embeddings with shape (batch_size, sequence_length, d_model)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenPositionEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, max_seq_len):\n",
        "        super(TokenPositionEmbedding, self).__init__()\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        # Create the position ids from 0 to max_seq_len - 1\n",
        "        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Get token and position embeddings\n",
        "        token_embeds = self.token_embedding(x)\n",
        "        position_embeds = self.position_embedding(position_ids)\n",
        "\n",
        "        # Combine token and position embeddings\n",
        "        embeddings = token_embeds + position_embeds\n",
        "\n",
        "        return self.dropout(embeddings)\n",
        "\n",
        "# Example usage:\n",
        "input_ids = torch.randint(0, vocab_size, (16, max_seq_len))  # 16 is batch_size\n",
        "\n",
        "embedding_layer = TokenPositionEmbedding(vocab_size, d_model, max_seq_len)\n",
        "embeddings = embedding_layer(input_ids)\n",
        "\n",
        "#Enumerate the TransformerBlock layers\n",
        "for i, layer in enumerate(embedding_layer.children()):\n",
        "    print(f\"Layer {i}: {layer}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706553256114
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is the purpose of the token and position embedding, and how is it different from a token embedding without a position embedding?\n",
        "\n",
        "### Token Embedding\n",
        "\n",
        "The concepts of token embeddings and position embeddings play crucial roles in processing sequential data like text. Let's explore each of these components:\n",
        "\n",
        "Token embeddings convert each token (like a word in a sentence) into a vector of fixed size. This vector representation captures the semantic information of the token, enabling the model to understand and process language.\n",
        "\n",
        "In practice, each unique token in the vocabulary is assigned a corresponding vector. These vectors are learned during the training process and are adjusted to encapsulate the meanings and relationships of words.\n",
        "\n",
        "If a transformer model uses only token embeddings, it would be able to understand the meaning of each word but not the order in which they appear. Language is inherently sequential, and the order of words affects the overall meaning of a sentence. Without position information, sentences with the same words in different orders would appear identical to the model.\n",
        "\n",
        "### Position Embedding\n",
        "\n",
        "Position embeddings are added to the model to give it an understanding of the order or position of words in a sequence. This is crucial for understanding the structure and meaning of sentences.\n",
        "\n",
        "Position embeddings are vectors that represent the position of each token in the sequence. These vectors are either learned during training or are predefined and based on mathematical functions (like sine and cosine functions).\n",
        "\n",
        "When combined with token embeddings, the model not only understands the meaning of each word but also the context provided by their order in the sentence. This combination allows the transformer to process sentences effectively, recognizing patterns and relationships that depend on the sequence of words.\n",
        "\n",
        "### Difference Between Token Embedding with and without Position Embedding\n",
        "\n",
        "Without position embeddings, the model loses the sequential context. It cannot differentiate between \"The cat sat on the mat\" and \"The mat sat on the cat,\" which have vastly different meanings.\n",
        "Handling of Sequential Data: Transformers are designed to handle sequential data, and position embeddings are crucial for maintaining the sequence information. Without position embeddings, transformers would be limited in their ability to process language effectively.\n",
        "\n",
        "In tasks like translation, question-answering, and text generation, understanding the order of words is essential. Position embeddings significantly enhance the transformer's performance in these tasks.\n",
        "\n",
        "### Summary\n",
        "While token embeddings provide meaning to individual words, position embeddings give the model an understanding of the order of those words, which is crucial for most language processing tasks. The combination of both allows transformers to effectively interpret and generate human language.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfomer Block\n",
        "This class takes as input the model dimension d_model, the number of attention heads num_heads, the feed-forward hidden dimension d_ff, the vocabulary size vocab_size, and the maximum sequence length max_seq_len. The forward method takes a tensor of shape (batch_size, sequence_length) with token ids and an optional mask, and it outputs the processed tensor with shape (batch_size, sequence_length, d_model)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, vocab_size, max_seq_len, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.embedding_layer = TokenPositionEmbedding(vocab_size, d_model, max_seq_len)\n",
        "\n",
        "        self.self_attention = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Token and position embedding\n",
        "        x = self.embedding_layer(x)\n",
        "\n",
        "        # Multi-head self-attention\n",
        "        attn_output, _ = self.self_attention(x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "\n",
        "        # Position-wise feed-forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "input_ids = torch.randint(0, vocab_size, (16, max_seq_len))  # 16 is batch_size\n",
        "\n",
        "transformer_block = TransformerBlock(d_model, num_heads, d_ff, vocab_size, max_seq_len)\n",
        "output = transformer_block(input_ids)\n",
        "\n",
        "#Enumerate the TransformerBlock layers\n",
        "for i, layer in enumerate(transformer_block.children()):\n",
        "    print(f\"Layer {i}: {layer}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706553332911
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the IMDB Data Set\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.float)\n",
        "\n",
        "def load_imdb_data(num_words, max_seq_len):\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "    # Pad sequences to max_seq_len\n",
        "    x_train = pad_sequences(x_train, maxlen=max_seq_len, padding='post', truncating='post')\n",
        "    x_test = pad_sequences(x_test, maxlen=max_seq_len, padding='post', truncating='post')\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "# Example usage:\n",
        "num_words = vocab_size\n",
        "batch_size = 16\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_imdb_data(num_words, max_seq_len)\n",
        "\n",
        "train_dataset = IMDBDataset(x_train, y_train)\n",
        "test_dataset = IMDBDataset(x_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706553396874
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Model\n",
        "\n",
        "Here's an example of building and training a transformer model using TransformerBlock, MultiHeadSelfAttention, TokenAndPositionEmbedding, and IMDBDataset from the previous examples. This example calculates and outputs the loss and accuracy for both training and test data for each epoch:\n",
        "\n",
        "This example creates a TransformerClassifier class that uses the TransformerBlock as the main component. The output of the transformer block is pooled along the sequence dimension using mean pooling before passing through a linear layer for classification.\n",
        "\n",
        "The training loop iterates through num_epochs and calculates the training and test loss and accuracy for each epoch. Note that the model should be set to train mode during training and eval mode during evaluation to enable/disable dropout and other regularization techniques correctly.\n",
        "\n",
        "The main components of the code are as follows:\n",
        "\n",
        "Loading the IMDB dataset: The load_imdb_data function is called to load the IMDB dataset, preprocess it by padding or truncating sequences to a fixed length (max_seq_len), and split it into training and testing sets.\n",
        "\n",
        "Creating Dataset and DataLoader instances: PyTorch Dataset and DataLoader instances are created for the training and validation sets. These will be used to iterate through the data during the training process.\n",
        "\n",
        "Defining the model: The TransformerClassifier class is created by combining the TransformerBlock with a fully connected layer for classification. This class is then instantiated using the hyperparameters, such as d_model, num_heads, and d_ff.\n",
        "\n",
        "Setting up the training loop: The model is trained for a specified number of epochs using the CrossEntropyLoss and the Adam optimizer. For each epoch, the model is trained on the training set and evaluated on the validation set. The loss and accuracy for both training and validation sets are calculated and printed for each epoch.\n",
        "\n",
        "In summary, this sample code demonstrates how to build, train, and evaluate a simple Transformer-based model for sentiment analysis on the Keras IMDB dataset. The model is trained using a single TransformerBlock and the performance metrics (loss and accuracy) are reported for each epoch.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, vocab_size, max_seq_len, num_classes, dropout=0.1):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "\n",
        "        self.transformer_block = TransformerBlock(d_model, num_heads, d_ff, vocab_size, max_seq_len, dropout)\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.transformer_block(x, mask)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        total += labels.size(0)\n",
        "        correct += ((outputs > 0) == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "    return running_loss / len(loader), correct / total\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.unsqueeze(1))\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            total += labels.size(0)\n",
        "            correct += ((outputs > 0) == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "    return running_loss / len(loader), correct / total\n",
        "\n",
        "# Model and training parameters\n",
        "num_classes = 1\n",
        "dropout = 0.1\n",
        "num_epochs = 10\n",
        "lr = 1e-4\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load data and create DataLoaders\n",
        "x_train, y_train, x_test, y_test = load_imdb_data(num_words, max_seq_len)\n",
        "train_dataset = IMDBDataset(x_train, y_train)\n",
        "test_dataset = IMDBDataset(x_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Create the model\n",
        "model = TransformerClassifier(d_model, num_heads, d_ff, vocab_size, max_seq_len, num_classes, dropout).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706553424128
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Enumerate the model layers\n",
        "\n",
        "for i, layer in enumerate(model.children()):\n",
        "    print(f\"Layer {i}: {layer}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.size()}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706553448933
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
        "          f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706554228071
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More info on Transformers\n",
        "\n",
        "![Transformer](Optimusprime-originaltoy.jpg)\n",
        "\n",
        "If you want more info on transformers, and some tutorials that _weren't_ generated by an AI, check out these links:\n",
        "\n",
        "## Keras tutorial:\n",
        "https://keras.io/examples/nlp/text_classification_with_transformer/\n",
        "\n",
        "## Other good tutorials:\n",
        "https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/\n",
        "\n",
        "https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
        "\n",
        "https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0\n",
        "\n",
        "https://www.tensorflow.org/text/tutorials/transformer\n",
        "\n",
        "https://www.kaggle.com/code/ritvik1909/text-classification-attention\n",
        "\n",
        "\n",
        "## General Overview:\n",
        "https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021\n",
        "\n",
        "https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter1/1?fw=pt\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}