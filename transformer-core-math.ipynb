{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Core Math Tutorial\n",
        "\n",
        "GPT, as well as other LLMs and LMMs are an amazing advance in AI.  But, how do they work?  They all use an ML model called a transformer.  Transformers allow AI to learn the complex relationships between tokens in the training data, in other words to learn the semantics, grammar, and even underlying knowledge encoded in natural language and images.  \n",
        "\n",
        "This tutorial will focus on the core math that makes a transformer block work, using multi headed attention as well as position and token embedding.  \n",
        "\n",
        "Both the descriptive explanations and the code samples for this tutorial were generated by chatGPT. In some cases the initial code had minor errors, these errors were also fixed by GPT 4 by feeding the errors back into GPT 4 and GPT 4 would generate new code.\n",
        "\n",
        "This is an advanced tutorial which builds the main components of the Transformer model, the multi headed attention mechanism and the position and token embedding, from scratch in PyTorch.\n",
        "\n",
        "Try using the following prompt to generate your own transformer tutorial.  There is alot of code to output and GPT can easily lose attention if the response is too long, so you may need to break the prompt up into smaller bits.  You can also ask followup questions to get it to explain how the code works.  Start with this prompt, and go from there:\n",
        "\n",
        "#### Prompt: \n",
        "```\n",
        "How can I build a transformer model for sentiment analysis using IMDB with multi headed attention and position and token embedding from scratch using pytorch\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "vocab_size = 20000\n",
        "max_seq_len = 200\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706896867404
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the IMDB Data Set\n",
        "\n",
        "The Keras IMDB dataset is a popular dataset for sentiment analysis tasks in natural language processing (NLP). It contains 50,000 movie reviews from the Internet Movie Database (IMDB) labeled as either positive (1) or negative (0) based on the sentiment expressed in the review. The dataset is divided into 25,000 reviews for training and 25,000 reviews for testing.\n",
        "\n",
        "The reviews in the dataset have been preprocessed, and each review is encoded as a sequence of word indices (integers). The indices represent the overall frequency rank of the words in the entire dataset. For instance, the integer \"3\" encodes the 3rd most frequent word in the data. This encoding allows for faster processing and less memory usage compared to working with raw text data.\n",
        "\n",
        "The Keras IMDB dataset is typically used for binary classification tasks, where the goal is to build a machine learning model that can predict whether a given movie review is positive or negative based on the text content. The dataset is accessible through the tensorflow.keras.datasets module in the TensorFlow library.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.float)\n",
        "\n",
        "def load_imdb_data(num_words, max_seq_len):\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "    # Pad sequences to max_seq_len\n",
        "    x_train = pad_sequences(x_train, maxlen=max_seq_len, padding='post', truncating='post')\n",
        "    x_test = pad_sequences(x_test, maxlen=max_seq_len, padding='post', truncating='post')\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "num_words = vocab_size\n",
        "batch_size = 16\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_imdb_data(num_words, max_seq_len)\n",
        "\n",
        "train_dataset = IMDBDataset(x_train, y_train)\n",
        "test_dataset = IMDBDataset(x_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "print (x_test.shape)\n",
        "print (y_test.shape)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"IMDB element {i} value: {x_test[i]}\")\n",
        "    print(f\"IMDB element {i} label: {y_test[i]}\\n\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706896871260
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token and Position Embedding\n",
        "\n",
        "This class takes as input the vocabulary size vocab_size, the model dimension d_model, and the maximum sequence length max_seq_len. The forward method takes a tensor of shape (batch_size, sequence_length) with token ids and outputs the combined token and position embeddings with shape (batch_size, sequence_length, d_model).\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenPositionEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, max_len=5000):\n",
        "        super(TokenPositionEmbedding, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = torch.zeros(max_len, embed_size)\n",
        "        \n",
        "        # Create position encoding\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2) * -(math.log(10000.0) / embed_size))\n",
        "        \n",
        "        self.positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        self.positional_encoding = self.positional_encoding.unsqueeze(0)\n",
        "        self.register_buffer('pe', self.positional_encoding, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_embedding(x) # (batch_size, seq_len, embed_size)\n",
        "        # Add positional encoding\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "#input_ids = torch.randint(0, vocab_size, (16, max_seq_len))  # 16 is batch_size\n",
        "input_ids = torch.from_numpy(x_test[:16])\n",
        "print(f\"input_ids.shape: {input_ids.shape}\")\n",
        "\n",
        "embedding_layer = TokenPositionEmbedding(vocab_size, d_model, max_seq_len)\n",
        "\n",
        "#Enumerate the TransformerBlock layers\n",
        "for i, layer in enumerate(embedding_layer.children()):\n",
        "    print(f\"Layer {i}: {layer}\")\n",
        "\n",
        "embeddings = embedding_layer(input_ids)\n",
        "print(f\"embeddings.shape: {embeddings.shape}\")\n",
        "print(f\"embeddings: {embeddings}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706896909540
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, seq_len = input_ids.size()\n",
        "        \n",
        "# Create the position ids from 0 to max_seq_len - 1\n",
        "position_ids = torch.arange(0, seq_len, dtype=torch.long, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "print(f\"position_ids.shape: {position_ids.shape}\")\n",
        "print(f\"position_ids: {position_ids}\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706896913791
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids1 = torch.randint(0, vocab_size, (16, max_seq_len))  # 16 is batch_size\n",
        "input_ids2 = torch.from_numpy(x_test[:16])\n",
        "print (type(input_ids1))\n",
        "print (type(input_ids2))\n",
        "print(f\"input_ids1.shape: {input_ids1.shape}\")\n",
        "print(f\"input_ids2.shape: {input_ids2.shape}\")\n",
        "\n",
        "print (type(input_ids1[0]))\n",
        "print (type(input_ids2[0]))\n",
        "print(f\"input_ids1.shape: {input_ids1[0].shape}\")\n",
        "print(f\"input_ids2.shape: {input_ids2[0].shape}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706897013950
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is the purpose of the token and position embedding, and how is it different from a token embedding without a position embedding?\n",
        "\n",
        "### Token Embedding\n",
        "\n",
        "The concepts of token embeddings and position embeddings play crucial roles in processing sequential data like text. Let's explore each of these components:\n",
        "\n",
        "Token embeddings convert each token (like a word in a sentence) into a vector of fixed size. This vector representation captures the semantic information of the token, enabling the model to understand and process language.\n",
        "\n",
        "In practice, each unique token in the vocabulary is assigned a corresponding vector. These vectors are learned during the training process and are adjusted to encapsulate the meanings and relationships of words.\n",
        "\n",
        "If a transformer model uses only token embeddings, it would be able to understand the meaning of each word but not the order in which they appear. Language is inherently sequential, and the order of words affects the overall meaning of a sentence. Without position information, sentences with the same words in different orders would appear identical to the model.\n",
        "\n",
        "### Position Embedding\n",
        "\n",
        "Position embeddings are added to the model to give it an understanding of the order or position of words in a sequence. This is crucial for understanding the structure and meaning of sentences.\n",
        "\n",
        "Position embeddings are vectors that represent the position of each token in the sequence. These vectors are either learned during training or are predefined and based on mathematical functions (like sine and cosine functions).\n",
        "\n",
        "When combined with token embeddings, the model not only understands the meaning of each word but also the context provided by their order in the sentence. This combination allows the transformer to process sentences effectively, recognizing patterns and relationships that depend on the sequence of words.\n",
        "\n",
        "### Difference Between Token Embedding with and without Position Embedding\n",
        "\n",
        "Without position embeddings, the model loses the sequential context. It cannot differentiate between \"The cat sat on the mat\" and \"The mat sat on the cat,\" which have vastly different meanings.\n",
        "Handling of Sequential Data: Transformers are designed to handle sequential data, and position embeddings are crucial for maintaining the sequence information. Without position embeddings, transformers would be limited in their ability to process language effectively.\n",
        "\n",
        "In tasks like translation, question-answering, and text generation, understanding the order of words is essential. Position embeddings significantly enhance the transformer's performance in these tasks.\n",
        "\n",
        "### Summary\n",
        "While token embeddings provide meaning to individual words, position embeddings give the model an understanding of the order of those words, which is crucial for most language processing tasks. The combination of both allows transformers to effectively interpret and generate human language.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Headed attention\n",
        "\n",
        "This class takes as input the model dimension d_model and the number of attention heads num_heads. The forward method takes a tensor of shape (batch_size, sequence_length, d_model) and an optional mask, and it outputs the context vectors and attention weights."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.w_queries = nn.Linear(d_model, d_model)\n",
        "        self.w_keys = nn.Linear(d_model, d_model)\n",
        "        self.w_values = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, queries, keys, values, mask=None):\n",
        "        attention_logits = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            attention_logits = attention_logits.masked_fill(mask == 0, float('-inf'))\n",
        "        attention_weights = F.softmax(attention_logits, dim=-1)\n",
        "        return torch.matmul(attention_weights, values), attention_weights\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        return x.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_len, _ = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        queries = self.split_heads(self.w_queries(x))\n",
        "        keys = self.split_heads(self.w_keys(x))\n",
        "        values = self.split_heads(self.w_values(x))\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "        context_vectors, attention_weights = self.scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        context_vectors = self.combine_heads(context_vectors)\n",
        "\n",
        "        return self.linear(context_vectors), attention_weights\n",
        "\n",
        "# Example usage:\n",
        "input_tensor = torch.rand(16, 50, d_model)  # 16 is batch_size and 50 is sequence length\n",
        "\n",
        "self_attention = MultiHeadSelfAttention(d_model, num_heads)\n",
        "output, attention_weights = self_attention(input_tensor)\n",
        "\n",
        "#Enumerate the MultiHeadSelfAttention layers\n",
        "for i, layer in enumerate(self_attention.children()):\n",
        "    print(f\"Layer {i}: {layer}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706897054400
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## What is the purpose of Queries, Keys, and Values and how are they different from a simple densely connected layer?\n",
        "\n",
        "The multi-head self-attention mechanism is a crucial component, characterized by three key elements: Queries (Q), Keys (K), and Values (V). Let's explore the purpose of each and how they differ from a simple densely connected (fully connected) neural network layer.\n",
        "\n",
        "### Queries (Q), Keys (K), and Values (V)\n",
        "\n",
        "1. **Queries (Q):** \n",
        "Represent the current word (or token) for which we are trying to establish its context and relationships with other words in the input sequence.\n",
        "\n",
        "1. **Keys (K):**\n",
        "Represent all words (or tokens) in the input sequence. The model uses them to determine how much focus or 'attention' each word in the sequence should get in relation to the current query word.\n",
        "\n",
        "1. **Values (V):**\n",
        "Also represent all words in the input sequence, but they are used to construct the output of the self-attention layer. The amount of attention a word gets influences how much its corresponding value contributes to the output.\n",
        "\n",
        "#### How They Work:\n",
        "\n",
        "In the self-attention mechanism, each word in the input sequence is initially transformed into Q, K, and V vectors through distinct linear transformations (learnable weights).\n",
        "The model calculates the attention scores by performing a dot product of the Q vector with all K vectors. These scores determine how much each word in the sequence should contribute to the representation of the current word.\n",
        "The attention scores are then used to create a weighted sum of the V vectors, which forms the output of the self-attention layer for each word.\n",
        "\n",
        "### Difference from a Densely Connected Layer:\n",
        "\n",
        "A densely connected layer learns a fixed transformation of its input data, applying the same transformation to all inputs. In contrast, the self-attention mechanism dynamically calculates how much each part of the input should contribute to the output based on the input data itself.\n",
        "\n",
        "The self-attention mechanism can capture relationships and dependencies between words in a sequence, regardless of their distance from each other. A densely connected layer lacks this contextual awareness and processes each input independently.\n",
        "\n",
        "Self-attention allows the model to focus on different parts of the input sequence differently for each output element, enabling a more nuanced and context-aware processing. Densely connected layers don't offer this level of flexibility as they apply the same transformation to all inputs.\n",
        "\n",
        "### Summary\n",
        "In a multi-head self-attention function, Queries, Keys, and Values are used to dynamically compute how different parts of the input sequence should be emphasized or 'attended to' for each element in the sequence. This differs from a simple densely connected layer, which lacks the ability to capture sequential and contextual relationships within the input data. Self-attention is inherently more flexible and context-aware, making it well-suited for tasks involving sequential data, like natural language processing."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What does the `split_heads` function do and how does it work?\n",
        "\n",
        "The multi-head self-attention mechanism involves a function often called split_heads or a similar variant. This function is essential for enabling the \"multi-head\" aspect of the self-attention. Let's delve into what this function does and how it works:\n",
        "\n",
        "### Purpose of `split_heads`\n",
        "The primary purpose of `split_heads` is to enable the model to simultaneously attend to information from different representation subspaces at different positions. By splitting the attention mechanism into multiple heads, the model can capture a richer variety of features in the input data.\n",
        "\n",
        "Each head in the multi-head attention can potentially focus on different aspects of the input data, allowing for parallel and diverse feature extraction. This leads to a more comprehensive understanding of the input.\n",
        "\n",
        "### How split_heads Works\n",
        "1. **Input to the Function:**\n",
        "    - The function typically takes the matrices Queries, Keys, and Values as inputs. Each of these matrices is the result of transforming the input sequence through different linear layers specific for Queries, Keys, and Values.\n",
        "\n",
        "1. **Reshaping the Matrices:**\n",
        "    - The `split_heads` function reshapes each of Queries, Keys, and Values matrices from their original shape `[batch_size, sequence_length, feature_dimension]` to a new shape `[batch_size, num_heads, sequence_length, feature_dimension/num_heads]`.\n",
        "\n",
        "    - This reshaping effectively splits the last dimension (feature_dimension) into two dimensions: the number of heads (num_heads) and the reduced feature dimension for each head.\n",
        "\n",
        "1. **Parallel Attention Processing:**\n",
        "\n",
        "    - After splitting, each head processes a slice of the original feature dimension, allowing the model to attend to different parts of the feature space independently and in parallel.\n",
        "    - This parallel processing enables the model to capture different types of relationships in the data, such as different aspects of semantic meaning in a language model.\n",
        "\n",
        "1. **Recombination and Output:**\n",
        "    - Once each head has processed its respective slice, the outputs are typically concatenated back together and passed through another linear layer to combine the information from all heads.\n",
        "\n",
        "    - This recombination ensures that the multi-head attention captures a wide range of information from the input while still being able to integrate these diverse signals.\n",
        "\n",
        "### Summary\n",
        "The split_heads function in a Transformer's multi-head self-attention mechanism plays a crucial role in diversifying the attention process. By splitting the Queries, Keys, and Values matrices into multiple heads, the Transformer can process the input data in parallel across different feature subspaces, enhancing its ability to capture complex patterns and relationships in the data. This functionality is fundamental to the Transformer architecture's success in various tasks like language understanding, translation, and generation."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How does `combine_heads` work?\n",
        "\n",
        "The combine_heads function plays a crucial role after the scaled dot product attention has been computed for each head. This function is essential for integrating the outputs from all heads back into a unified representation. Let's explore what this function does and how it operates:\n",
        "\n",
        "### Purpose of `combine_heads`\n",
        "1. **Aggregating Outputs from Multiple Heads:** The main purpose of `combine_heads` is to merge the outputs from each of the attention heads. Since each head captures different aspects or features of the input data, combining them allows the model to consider all these aspects simultaneously.\n",
        "\n",
        "1. **Restoring Original Dimensionality:** The function also serves to reshape the output back to the original embedding dimensionality. This is necessary for maintaining consistency in the network's layers and for subsequent processing.\n",
        "\n",
        "### How `combine_heads` Works\n",
        "1. **Input to the Function:** The function typically receives the outputs from the attention heads, where each head has produced an output matrix of shape `[batch_size, sequence_length, feature_dimension/num_heads]`.\n",
        "\n",
        "1. **Concatenating the Outputs:**\n",
        "The outputs from all the heads are concatenated along the dimension that represents the feature space. This concatenation effectively reverses the operation performed by the split_heads function.\n",
        "After concatenation, the shape of the resulting matrix is `[batch_size, sequence_length, feature_dimension]`, where `feature_dimension` is typically the original embedding size.\n",
        "\n",
        "1. **Preparing for Subsequent Layers:**\n",
        "The output of `combine_heads` is now in a suitable format to be passed on to the next layer in the Transformer, such as a feed-forward neural network layer.\n",
        "This step is crucial for ensuring that the sequential processing in the Transformer architecture is maintained.\n",
        "### Summary\n",
        "The combine_heads function in a Transformer's multi-head self-attention mechanism is integral for integrating the diverse outputs from each attention head. By concatenating and optionally transforming these outputs, the function provides a comprehensive representation that encapsulates the varied features captured by each head. This step is key to the Transformer's ability to process and understand complex patterns in data, particularly in tasks involving sequential or structured data like natural language processing."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is the `scaled_dot_product_attention` function do and how does it work\n",
        "\n",
        "The `scaled_dot_product_attention` function is a critical component. It computes the attention weights and produces a weighted sum of the values. This function is where the actual 'attention' part of the mechanism takes place. Let's explore what this function does and how it operates:\n",
        "\n",
        "### Purpose of `scaled_dot_product_attention`\n",
        "1. **Computing Attention Weights:** The primary purpose of this function is to calculate how much attention each element of the sequence should pay to every other element. It's about determining the relevance or importance of all other tokens in the sequence for a given token.\n",
        "\n",
        "1. **Producing Contextualized Representations:** By computing these attention weights and applying them to the values, the function produces a new set of vectors that are contextually informed. These vectors represent each token not just as itself, but as a summary of how it relates to every other token in the sequence.\n",
        "\n",
        "### How scaled_dot_product_attention Works\n",
        "1. **Inputs to the Function:** The function typically takes three inputs: Queries (Q), Keys (K), and Values (V). Optionally, a mask may also be provided to exclude certain positions from attention (like padding tokens).\n",
        "\n",
        "1. **Calculating Dot Products of Queries and Keys:**\n",
        "The function starts by computing the dot product between each query and all keys. This operation essentially measures the similarity or compatibility between each query and key pair.\n",
        "The resulting matrix of dot products has a shape `[sequence_length, sequence_length]`, representing attention scores for each pair of tokens in the sequence.\n",
        "1. **Scaling the Dot Products:**\n",
        "The dot products are scaled down by the square root of the dimension of the key vectors. This scaling is done to prevent the softmax function (applied in the next step) from having a too-small gradient, which can happen when the dot products are large. The scaling helps in stabilizing the gradient descent algorithm during training.\n",
        "\n",
        "1. **Applying Softmax: **\n",
        "A softmax function is applied to each row of the scaled dot product matrix. The softmax function converts the raw scores into probabilities, which sum up to 1. This step determines the final attention weights.\n",
        "The softmax is often applied after masking, ensuring that positions to be ignored (like padding) receive zero weight.\n",
        "1. **Multiplying with the Values:**\n",
        "The attention weights are then used to create a weighted sum of the value vectors. This step effectively selects or highlights the information in the values based on the computed attention weights.\n",
        "The output is a new set of vectors, each representing a token in the sequence, reweighted to include information from other relevant tokens.\n",
        "1. **Output of the Function:**\n",
        "The output is a matrix of the same shape as the values matrix, representing the input sequence where each element now has contextual information from the entire sequence.\n",
        "### Summary\n",
        "The `scaled_dot_product_attention` function is at the heart of the self-attention mechanism in Transformers. It enables the model to focus on different parts of the input sequence in a context-sensitive manner. By calculating attention weights and applying them to the values, this function produces output vectors that are contextualized representations of each input token, taking into account the entire sequence. This sophisticated attention mechanism is a key reason for the effectiveness of Transformers in tasks that require an understanding of the entire context, such as natural language processing and sequence modeling."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  What is the purpose of the Linear layer\n",
        "\n",
        "In a Transformer's multi-head self-attention mechanism, the fourth layer, commonly referred to as the fully connected layer (fc) or sometimes as a linear layer, plays a vital role in integrating and refining the outputs from the self-attention process. Let's break down its purpose:\n",
        "\n",
        "### Purpose of the linear layer (fc)\n",
        "1. **Integration of Attention Heads:**\n",
        "After the self-attention mechanism processes the input through multiple heads, the results from each head need to be integrated. The linear layer serves to combine these diverse attention outputs into a single, unified output.\n",
        "\n",
        "1. **Transformation of Concatenated Outputs:**\n",
        "The outputs of the multiple attention heads are concatenated to form a single matrix. The linear layer then applies a linear transformation to this concatenated matrix. This step is crucial for mapping the combined, multi-dimensional attention information back into the original input space (or to a desired output dimensionality).\n",
        "\n",
        "1. **Maintaining Depth of Representation:**\n",
        "The linear layer (fc) ensures that the depth of the model's representation (i.e., the dimensionality of the feature space) is maintained or appropriately transformed. This consistency is essential for stacking multiple layers of the Transformer, allowing each layer to build upon the transformed representations of the previous layer.\n",
        "\n",
        "1. **Adding Learnable Parameters:**\n",
        "The linear layer (fc) introduces additional learnable parameters to the model. These parameters are optimized during training, allowing the model to better integrate and interpret the information gleaned from the multiple attention heads.\n",
        "\n",
        "1. **Enhancing Model's Capacity:** By combining and transforming the outputs of the attention heads, the linear layer (fc) enhances the model's capacity to capture complex patterns and relationships in the data. This step is critical for the overall performance of the Transformer in tasks like language understanding and generation.\n",
        "\n",
        "### How the linear layer (fc) Layer Works\n",
        "- **Linear Transformation:** The linear layer (fc) typically performs a linear transformation. It takes the concatenated outputs from the attention heads and multiplies them with a weight matrix (learnable parameters), often followed by adding a bias term.\n",
        "\n",
        "- **Dimensionality Management:** The linear layer (fc) can either preserve the dimensionality of the input or transform it to a different dimensionality, depending on the design of the Transformer model. This flexibility allows the model to be tailored to specific tasks or requirements.\n",
        "\n",
        "### Summary\n",
        "The linear layer (fc) in a transformer's multi-head self-attention mechanism serves as a critical component for integrating, transforming, and refining the outputs from the attention heads. It adds depth and capacity to the model, enabling complex feature integration and aiding in the model's overall ability to process and understand sequential data effectively."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfomer Block\n",
        "This class takes as input the model dimension d_model, the number of attention heads num_heads, the feed-forward hidden dimension d_ff, the vocabulary size vocab_size, and the maximum sequence length max_seq_len. The forward method takes a tensor of shape (batch_size, sequence_length) with token ids and an optional mask, and it outputs the processed tensor with shape (batch_size, sequence_length, d_model)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, vocab_size, max_seq_len, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.embedding_layer = TokenPositionEmbedding(vocab_size, d_model, max_seq_len)\n",
        "\n",
        "        self.self_attention = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Token and position embedding\n",
        "        x = self.embedding_layer(x)\n",
        "\n",
        "        # Multi-head self-attention\n",
        "        attn_output, _ = self.self_attention(x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "\n",
        "        # Position-wise feed-forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "input_ids = torch.randint(0, vocab_size, (16, max_seq_len))  # 16 is batch_size\n",
        "\n",
        "transformer_block = TransformerBlock(d_model, num_heads, d_ff, vocab_size, max_seq_len)\n",
        "output = transformer_block(input_ids)\n",
        "\n",
        "#Enumerate the TransformerBlock layers\n",
        "for i, layer in enumerate(transformer_block.children()):\n",
        "    print(f\"Layer {i}: {layer}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706897061288
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Model\n",
        "\n",
        "Here's an example of building and training a transformer model using TransformerBlock, MultiHeadSelfAttention, TokenAndPositionEmbedding, and IMDBDataset from the previous examples. This example calculates and outputs the loss and accuracy for both training and test data for each epoch:\n",
        "\n",
        "This example creates a TransformerClassifier class that uses the TransformerBlock as the main component. The output of the transformer block is pooled along the sequence dimension using mean pooling before passing through a linear layer for classification.\n",
        "\n",
        "The training loop iterates through num_epochs and calculates the training and test loss and accuracy for each epoch. Note that the model should be set to train mode during training and eval mode during evaluation to enable/disable dropout and other regularization techniques correctly.\n",
        "\n",
        "The main components of the code are as follows:\n",
        "\n",
        "Loading the IMDB dataset: The load_imdb_data function is called to load the IMDB dataset, preprocess it by padding or truncating sequences to a fixed length (max_seq_len), and split it into training and testing sets.\n",
        "\n",
        "Creating Dataset and DataLoader instances: PyTorch Dataset and DataLoader instances are created for the training and validation sets. These will be used to iterate through the data during the training process.\n",
        "\n",
        "Defining the model: The TransformerClassifier class is created by combining the TransformerBlock with a fully connected layer for classification. This class is then instantiated using the hyperparameters, such as d_model, num_heads, and d_ff.\n",
        "\n",
        "Setting up the training loop: The model is trained for a specified number of epochs using the CrossEntropyLoss and the Adam optimizer. For each epoch, the model is trained on the training set and evaluated on the validation set. The loss and accuracy for both training and validation sets are calculated and printed for each epoch.\n",
        "\n",
        "In summary, this sample code demonstrates how to build, train, and evaluate a simple Transformer-based model for sentiment analysis on the Keras IMDB dataset. The model is trained using a single TransformerBlock and the performance metrics (loss and accuracy) are reported for each epoch.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, vocab_size, max_seq_len, num_classes, dropout=0.1):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "\n",
        "        self.transformer_block = TransformerBlock(d_model, num_heads, d_ff, vocab_size, max_seq_len, dropout)\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.transformer_block(x, mask)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        total += labels.size(0)\n",
        "        correct += ((outputs > 0) == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "    return running_loss / len(loader), correct / total\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.unsqueeze(1))\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            total += labels.size(0)\n",
        "            correct += ((outputs > 0) == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "    return running_loss / len(loader), correct / total\n",
        "\n",
        "# Model and training parameters\n",
        "num_classes = 1\n",
        "dropout = 0.1\n",
        "num_epochs = 10\n",
        "lr = 1e-4\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load data and create DataLoaders\n",
        "x_train, y_train, x_test, y_test = load_imdb_data(num_words, max_seq_len)\n",
        "train_dataset = IMDBDataset(x_train, y_train)\n",
        "test_dataset = IMDBDataset(x_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Create the model\n",
        "model = TransformerClassifier(d_model, num_heads, d_ff, vocab_size, max_seq_len, num_classes, dropout).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706897071525
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Enumerate the model layers\n",
        "\n",
        "for i, layer in enumerate(model.children()):\n",
        "    print(f\"Layer {i}: {layer}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.size()}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706897071767
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
        "          f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706897121760
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More info on Transformers\n",
        "\n",
        "If you want more info on transformers, and some tutorials that _weren't_ generated by an AI, check out these links:\n",
        "\n",
        "## Deep Learning\n",
        "This is an authoritative treatment of deep learning:\n",
        "[Deep Learning PDF - Ian Goodfellow, Yoshua Bengio and Aaron Courville](https://github.com/janishar/mit-deep-learning-book-pdf/blob/master/complete-book-pdf/deeplearningbook.pdf)\n",
        "\n",
        "## Keras tutorial:\n",
        "https://keras.io/examples/nlp/text_classification_with_transformer/\n",
        "\n",
        "## Other good tutorials:\n",
        "https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/\n",
        "\n",
        "https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
        "\n",
        "https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0\n",
        "\n",
        "https://www.tensorflow.org/text/tutorials/transformer\n",
        "\n",
        "https://www.kaggle.com/code/ritvik1909/text-classification-attention\n",
        "\n",
        "\n",
        "## General Overview:\n",
        "https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021\n",
        "\n",
        "https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter1/1?fw=pt\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}